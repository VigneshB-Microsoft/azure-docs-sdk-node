### YamlMime:TSTypeAlias
name: LexicalTokenizerName
uid: '@azure/search-documents.LexicalTokenizerName'
package: '@azure/search-documents'
summary: >-
  Defines values for LexicalTokenizerName. \

  <xref:KnownLexicalTokenizerName> can be used interchangeably with
  LexicalTokenizerName,
   this enum contains the known values that the service supports.
  ### Known values supported by the service

  **classic**: Grammar-based tokenizer that is suitable for processing most
  European-language documents. See
  http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicTokenizer.html
  \

  **edgeNGram**: Tokenizes the input from an edge into n-grams of the given
  size(s). See
  https://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.html
  \

  **keyword_v2**: Emits the entire input as a single token. See
  http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/KeywordTokenizer.html
  \

  **letter**: Divides text at non-letters. See
  http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LetterTokenizer.html
  \

  **lowercase**: Divides text at non-letters and converts them to lower case.
  See
  http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LowerCaseTokenizer.html
  \

  **microsoft_language_tokenizer**: Divides text using language-specific rules.
  \

  **microsoft_language_stemming_tokenizer**: Divides text using
  language-specific rules and reduces words to their base forms. \

  **nGram**: Tokenizes the input into n-grams of the given size(s). See
  http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html
  \

  **path_hierarchy_v2**: Tokenizer for path-like hierarchies. See
  http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/path/PathHierarchyTokenizer.html
  \

  **pattern**: Tokenizer that uses regex pattern matching to construct distinct
  tokens. See
  http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/pattern/PatternTokenizer.html
  \

  **standard_v2**: Standard Lucene analyzer; Composed of the standard tokenizer,
  lowercase filter and stop filter. See
  http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html
  \

  **uax_url_email**: Tokenizes urls and emails as one token. See
  http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.html
  \

  **whitespace**: Divides text at whitespace. See
  http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/WhitespaceTokenizer.html
fullName: LexicalTokenizerName
remarks: ''
isDeprecated: false
syntax: |
  type LexicalTokenizerName = string
